Machine Learning:

- Supervised Learning:
  - Regression problems: continuous values with aim at predicting where it will fall on the graph
  - Classification problems: mostly with discrete values, determining where a value falls in.
  
- Unsupervised Learning:
  - No structured/known clusters. You find the clusters yourself. E.g: finding similar news articles
    on the web, clustering users in a database according to their preferences.
  - Harder to know what exactly you're looking for

  
- Linear Regression:
  - Form of Supervised learning - given a 'right answer' for each example in the data
  - given data is also referred to as "Training set"
    Notation: m - no. of training examples, x's - input vars/features, y's - output vars/target vars
      (x,y)- one training example
      Training set -> Learning algorithm -> Hypothesis(h); 
      h maps from input x to estimated y
      h(x) = thet0 + thet1x (simple linear graph);
      best way to prediction is the one that reduces: h(x) - y
      Cost Function:
        One solution: minimize(thet0, thet1) = (0.5/n)*Sum(h(x*)-y*)^2 where * = range(0,n), n = no. of training sets
         we are minimizing, so 0.5n is a constant - dividing by n to ensure we get an average (lots of sets vs few)
        also known as square cost function. Used in most regression probs
        Cost fn sually abbrev. by J(thet0, thet1)
        J(thet1) looks like a quadratic function (under perfect conditions)
        Mapping cost fn (J), thet0, & thet1 you get an inverted cone
        Next step is finding an algorithm that calculates thet0 & thet1 to minimize the cost fn.
			Gradient Descent:
        For minimizing Cost fn (used to minimize other fns too)
				Initialize thet0 & thet1 both to 0.
				Taking incremental steps from the initial position (looking for the lowest position around you).
				Keep doing the same until you reach a local minimum
				Algorithm: 
					thet(j) := thet(j) - alpha(delta J(thet0,thet1)/delta(thet(j))); [for j=0 and j=1]
						":=" -> assign LHS to RHS
						alpha -> learning rate (How big a step we take)
						delta -> partial derivatives for J(thet0,thet1)
						[for j=0 & j=1] -> simultaneously update thet0 & thet1
							temp0 = thet0 - alpha(...)
							temp1 = thet1 - alpha(...)
							thet0 = temp0
							thet1 = temp1
							NOT DOING this correctly (i.e: calculating, update, calculate, update) will lead to using unique J's
								check the algorithm above
						alpha is always positive (we are trying to reach the lowest local minimum)
						remember, we don't care about the size of thet(j). Just the local minima.
						If alpha is too small, we'll take too many steps. If it's too big, we'll risk passing the minima.
						However, the slope (i.e partial deriv) helps to ensure thet(j) is updated appropriately when nearing
							the minima (deriv gets smaller)
				Combining the J(thet0,thet1) & h(x) into the Gradient Descent,
					j=0 -> thet0 := thet0 - (alpha/n)(Sum(h(x*)-y*));
					j=1 -> thet1 := thet1 - (alpha/n)(Sum((h(x*)-y*)(x*))); Don't forget to update simultaneously
				Gradient descent always ends up being a convex function, i.e. only one global optima (no local minima)
				'Batch' Grad. desc. means we are using all training sets (for this kind of linear regressions)
				
- Multiple Variables:
	- Notation: x(i) = input of i-th training example
		x(i)j = value of feature j in i-th training eg.
		n = number of features
		Hypothesis equation: h(x) = thet0 + thet1x1 + ... + thetnxn
			For convenience: h(x) = thet(transpose)x
				because, in matrix form, theta matrix transposed when multiplied with the matrix of x-values
				will return the hypothesis equation.
			
	- Feature scaling:
		For gradient descent of multiple variables, if plotting against two variables, the gradient descent
		can take an extreme oval shape. This is due to significant variation in value of one variables compared
		to the other. This usually leads to inefficiency when finding the optimum point (takes longer).Best
		solution is to scale the values to be somewhat similar (e.g: dividing prices by thousands to get smaller
		numbers).
		Generally: get every feature to approximately -1 <= x <= 1 range (this is subjective!).
		Mean normalization: 
		Trying to get the features to have an average of 0. Max & min values sitting on either end
			replace x with x - average
		Remember some important points:
			- beware of the value of alpha:
				you can set it to 0.001 - 0.003 - 0.01 - 0.03 - 0.1 => this way you are multiplying by 3 (roughly)
			- Polynomial Regression: 
				You can play with variables. E.g: squaring the size of the house, etc etc just to see what model
				you get and how well it can be used to predict the price.
				
				



				
				
				
				
				
      